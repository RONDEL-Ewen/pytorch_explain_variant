\documentclass[sigconf, nonacm]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}



%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Explainable and Trustworthy AI - Project]{}{}{}



\begin{document}

\title{P3 - Exploring linear equations as means for a more interpretable decision with Deep Concept Reasoning}

%%
%% The "author" 
\author{Ewen Rondel}
\affiliation{%
  \text{s321468}
  \\
  \institution{Politecnico di Torino}
  \city{Turin}
  \country{Italy}
}

\author{Jo√£o Almeida}
\affiliation{%
  \text{s332204}
  \\
  \institution{Politecnico di Torino}
  \city{Turin}
  \country{Italy}
}
\author{Thomas Lepretre}
\affiliation{%
  \text{s332585}
  \\
  \institution{Politecnico di Torino}
  \city{Turin}
  \country{Italy}
}



%%
\begin{abstract}
After exploring different ideas for enhancing the interpretability of concept-based models, we propose a novel variant of the Deep Concept Reasoning (DCR) model, focusing on utilizing linear equations to achieve interpretable predictions. The traditional Concept Embedding Model (CEM) offers robust generalization capabilities akin to black-box models, but lacks interpretability due to its high-dimensional concept embeddings. Conversely, the DCR model provides interpretable predictions by generating logic rules over concept embeddings, yet its interpretability paradigm can be restrictive.

Our approach aims to simplify and enhance the interpretability of the DCR model by employing linear equations to represent concept embeddings. This modification maintains the end-to-end trainability and predictive power of DCR while offering a more straightforward, globally interpretable prediction mechanism. Reviewing the previous work on supervised concept-based models, we emphasize the methodologies and limitations of CEM and DCR.

The proposed model is implemented and rigorously tested on both synthetic and real-world datasets. We evaluate its performance against standard DCR using various metrics, focusing on generalization capability and interpretability. Preliminary results indicate that our model not only retains the predictive accuracy of DCR but also significantly improves the clarity and simplicity of the interpretability, making it a promising tool for applications requiring transparent decision-making processes.
\end{abstract}



%% OPTIONAL 
%% Keywords. The author(s) should pick words that accurately describe the work being presented. Separate the keywords with commas.
\keywords{}



%%
%% This command processes the author and affiliation and title information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
In recent years, the field of Explainable Artificial Intelligence (XAI) has gained significant attention as machine learning models become increasingly complex and ubiquitous in decision-making processes. While these advanced models offer impressive predictive capabilities, their lack of transparency often hinders their adoption in critical domains where interpretability is paramount. Concept-based models have emerged as a promising approach to bridge the gap between high performance and interpretability in machine learning.
The evolution of concept-based models has seen several key developments, from the Concept Bottleneck Model (CBM) \cite{koh2020concept} to the Concept Embedding Model (CEM) \cite{espinosa2022concept}, and most recently, the Deep Concept Reasoning (DCR) model \cite{barbiero2023interpretable}. Each iteration has aimed to improve upon its predecessors, balancing, and with DCR, outright challenging, the trade-off between generalization capability and interpretability. However, further improvements in achieving clear, intuitive explanations for model decisions without sacrificing high performance can still be explored.
Our research focuses on developing a novel variant of the DCR model that maintains its end-to-end trainability and predictive power while enhancing interpretability through the use of linear equations. This approach aims to simplify the interpretation of concept embeddings, providing a more accessible and globally interpretable prediction mechanism compared to the logic rules employed in the original DCR model.
The primary objectives of this study are to:

\begin{enumerate}
    \item Analyze the strengths and limitations of existing supervised concept-based models, with a particular focus on CEM and DCR.
    \item Propose and implement a new model that utilizes linear equations for interpretable predictions over concept embeddings.
    \item Evaluate the performance and interpretability of our proposed model against the standard DCR model using both synthetic and real-world datasets.
\end{enumerate}

With these objectives in mind, we aim to contribute to the ongoing development of explainable AI systems that can be reliably deployed in scenarios requiring both high performance and transparent decision-making processes, by further pushing the ease of interpretability of the models.

\section{Related work}
The field of concept-based models in machine learning has seen rapid advancement in recent years, with each new model building upon the strengths of its predecessors while addressing their limitations. This section provides an overview of the key developments leading to our proposed model.
\subsection{Concept Bottleneck Model (CBM)}
Koh et al. introduced the Concept Bottleneck Model in 2020 \cite{koh2020concept}, marking a significant step towards interpretable machine learning. CBM structures the prediction process into two distinct stages: concept prediction and task prediction. By forcing the model to make predictions through an intermediate layer of human-specified concepts, CBM provides a level of transparency to the decision-making process. However, the use of single neurons to represent concepts limits the model's representation capability, often resulting in a trade-off between interpretability and predictive performance.
\subsection{Concept Embedding Model (CEM)}
To address the limitations of CBM, Zarlenga et al. proposed the Concept Embedding Model in 2022 \cite{espinosa2022concept}. CEM represents concepts as embeddings rather than single neurons, significantly enhancing the model's representation capability. This approach allows CEM to achieve generalization performance comparable to black-box end-to-end models while retaining the ability to interact with the model through concepts. However, the high-dimensional nature of concept embeddings in CEM makes it challenging to interpret the model's decisions, as the individual dimensions of these embeddings lack clear semantic meaning.
\subsection{Deep Concept Reasoning (DCR)}
Building upon the strengths of CEM, Barbiero et al. introduced the Deep Concept Reasoning model in 2023 \cite{barbiero2023interpretable}. DCR is an end-to-end trainable model that provides interpretable predictions in the form of logic rules over concept embeddings. For each sample, DCR predicts a rule that holds true for the given sample's concept embeddings, which is then symbolically executed over the concept scores. This approach offers a novel way to interpret the model's decisions, bridging the gap between the high performance of concept embeddings and the interpretability desired in many applications.

\section{Research gaps}
While the DCR model represents a significant advancement in interpretable concept-based models, some current limitations may offer opportunities for improvement:

\subsection{Complexity of Logic Rules}
In situations where a large number of concepts are necessary to differentiate between tasks, DCR may generate increasingly complex rules. Although this issue is not prevalent in most current benchmark datasets and real-world applications, it highlights a potential scalability concern for more intricate problem domains. Developing methods to manage rule complexity in high-dimensional concept spaces could enhance the model's applicability to a broader range of problems.
\subsection{Global Interpretability}
A key limitation of DCR is the potential misalignment between its global behavior and the rules it generates. This discrepancy can pose challenges in scenarios where a comprehensive understanding of the model's overall decision-making process is crucial. Enhancing the model's ability to provide accurate global interpretations without sacrificing local explanatory power remains an open challenge.
\subsection{Dependency on Concept Embeddings}
DCR's reliance on concept embeddings as inputs presupposes the availability of concept-based datasets or robust concept-discovery techniques. This dependency may limit the model's applicability in domains where such resources are scarce or where concept identification is particularly challenging. Exploring ways to integrate concept discovery within the DCR framework or developing methods to operate with less refined concept representations could broaden the model's usefulness.
\subsection{Trade-off Between Expressiveness and Simplicity}
While logic rules offer high expressiveness, they may not always be the most concise or intuitive way to represent relationships between concepts and predictions. There's an opportunity to explore alternative representation methods that balance expressiveness with simplicity, potentially making the model's decisions more accessible to a wider audience.

By addressing these research gaps, particularly the complexity of logic rules, we aim to develop a variant of DCR that maintains its strengths in concept-based reasoning while offering more intuitive and widely accessible interpretations. Our proposed approach using linear equations seeks to tackle these challenges, especially in simplifying the interpretability mechanism without sacrificing the model's predictive power.

\section{Methodology}
Our proposed variant of the Deep Concept Reasoning (DCR) model aims to enhance interpretability while maintaining the model's predictive power. We achieve this by replacing the complex fuzzy logic rules used in the original DCR with a more intuitive linear equation approach.

\subsection{Model Architecture}
The core of our model remains similar to the original DCR, consisting of three main components:
\begin{enumerate}
    \item Encoder: Encodes the input in some way, the architecture of this component will vary depending on the dataset and problem being handled.
    \item Concept Embedding: Creates the concept embeddings to be used by DCR using the encoded input.
    \item Concept Reasoning: Creates a linear equation using the concept rules as to generate a prediction from them.
\end{enumerate}

The only modification in our approach lies in the Concept Reasoning Layer, which we have redesigned to use linear equations instead of fuzzy logic rules.

\subsection{Linear Equation Concept Reasoning}
In our variant, the Concept Reasoning Layer implements a linear transformation of the concept embeddings to generate class predictions. This approach can be represented by the following equation:

\begin{equation}
    y = \sigma(Wx + b)
\end{equation}

Where:
\begin{itemize}
    \item $y$ is the vector of class predictions
    \item $\sigma$ is the sigmoid activation function
    \item $W$ is the weight matrix
    \item $x$ is the flattened concept embedding vector
    \item $b$ is the bias term
\end{itemize}

This linear approach offers several advantages:

\begin{enumerate}
    \item Simplicity: Linear equations can be more intuitive and easier to interpret than logic rules which can be complex depending on the domain.
    \item Scalability: The linear model can handle a large number of concepts without exponentially increasing complexity.
    \item Continuous representation: Unlike binary logic rules, linear equations can capture nuanced relationships between concepts and classes.
\end{enumerate}

\subsection{Interpretability Mechanism}

Our model provides interpretable predictions through an analysis of the weights in the linear transformation. For each class, we can express the prediction as a weighted sum of concept contributions:

\begin{equation}
    \text{class\_score} = w_1 \cdot c_1 + w_2 \cdot c_2 + \ldots + w_n \cdot c_n
\end{equation}

Where $w_i$ represents the weight associated with concept $c_i$.

To enhance interpretability, we normalize these weights to show the relative importance of each concept in the decision-making process. For each class, we calculate:

\begin{enumerate}
    \item The total absolute weight: $\sum_{i=1}^n |w_i|$
    \item The percentage contribution of each concept: $\frac{|w_i|}{\sum_{i=1}^n |w_i|} \cdot 100\%$
\end{enumerate}

This allows us to present explanations in the form of linear equations, where each term includes:
\begin{itemize}
    \item The weight of the concept
    \item The concept name
    \item The sign of the contribution (positive or negative)
    \item The percentage contribution to the overall decision
\end{itemize}

\subsection{Global vs. Local Interpretability}

Unlike the original DCR, which focused on local interpretability through sample-specific logic rules, our approach naturally provides both local and global interpretability:

\begin{enumerate}
    \item \textbf{Global Interpretability:} The weights of the linear transformation offer a clear, consistent explanation of the model's overall behavior across all samples.
    \item \textbf{Local Interpretability:} By combining the global weights with the specific concept embeddings or predictions for a given sample, we can generate sample-specific explanations.
\end{enumerate}

This dual approach to interpretability addresses one of the key limitations of the original DCR model.

\subsection{Training and Optimization}

The training process for our model remains end-to-end, similar to the original DCR. We use backpropagation to optimize all components of the model simultaneously, including the concept encoder, concept predictor, and our modified concept reasoning layer.

The loss function combines:
\begin{enumerate}
    \item A concept prediction loss (e.g., binary cross-entropy for each concept)
    \item A classification loss for the final predictions
\end{enumerate}

This ensures that the model learns meaningful concept representations while optimizing for the target task.

\subsection{Summary}

In summary, our methodology preserves the concept-based approach of DCR while simplifying the reasoning mechanism through linear equations. This modification aims to enhance interpretability, scalability, and the balance between global and local explanations, addressing several of the research gaps identified in the original DCR model.

\section{Experiments and analysis}
[Presentation of the experimental results and their discussion]



\section{Conclusions}
[Brief conclusion summarizing the main outcomes of the project]


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}


\end{document}
\endinput
%%